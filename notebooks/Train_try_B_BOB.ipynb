{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from My_Pka_Model import Pka_basic,Pka_acidic\n",
    "import torch\n",
    "\n",
    "import dgl\n",
    "import dgllife\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 27 08:09:24 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    34W / 250W |   2022MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:2F:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    34W / 250W |   1395MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    34W / 250W |   1791MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    34W / 250W |   1763MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     28086      C   .../xjc/anaconda/envs/rdkit-env/bin/python  1005MiB |\n",
      "|    0     28164      C   .../xjc/anaconda/envs/rdkit-env/bin/python  1005MiB |\n",
      "|    1     24245      C   .../xjc/anaconda/envs/rdkit-env/bin/python  1383MiB |\n",
      "|    2     39996      C   python                                      1779MiB |\n",
      "|    3     38538      C   python                                      1751MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed) \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  \n",
    "np.random.seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(labels)\n",
    "\n",
    "node_featurizer = CanonicalAtomFeaturizer(atom_data_field='h')\n",
    "edge_featurizer = CanonicalBondFeaturizer(bond_data_field='h')\n",
    "\n",
    "def load_data(file_name,batch_size = 128,shuffle = True,split_ratio = False):\n",
    "    dataset = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.replace('\\n','').split('\\t')\n",
    "            g = smiles_to_bigraph(smiles=line[0], \n",
    "                          node_featurizer=node_featurizer,\n",
    "                          edge_featurizer=edge_featurizer,\n",
    "                          canonical_atom_order= False)\n",
    "            dataset.append((g,float(line[1])))\n",
    "            \n",
    "    if split_ratio == False:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,collate_fn=collate)\n",
    "        return dataloader\n",
    "    else:\n",
    "        random.shuffle(dataset)\n",
    "        length = len(dataset)\n",
    "        dataloader_list = []\n",
    "        for i in split_ratio:\n",
    "            num = round(length * i)\n",
    "            dataset_part = dataset[:num]\n",
    "            dataset = dataset[num:]\n",
    "            dataloader = DataLoader(dataset_part, batch_size=batch_size, shuffle=shuffle,collate_fn=collate)\n",
    "            dataloader_list.append(dataloader)\n",
    "        dataset_part = dataset\n",
    "        dataloader = DataLoader(dataset_part, batch_size=batch_size, shuffle=shuffle,collate_fn=collate)\n",
    "        dataloader_list.append(dataloader)\n",
    "        return dataloader_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epoch_num = 1000\n",
    "layer_num = 6\n",
    "learning_rate = 0.0003\n",
    "weight_decay = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pka_acidic(node_feat_size = 74,\n",
    "                   edge_feat_size = 12,\n",
    "                   output_size = 1,\n",
    "                   num_layers= layer_num,\n",
    "                   graph_feat_size=200,\n",
    "                   dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_data('./Dataset/acidic_non_B_smiles_BOB.txt',batch_size = batch_size)\n",
    "val_loader = load_data('./Dataset/BOB_smiles.txt',batch_size = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9015\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset))\n",
    "print(len(val_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\ttrain_RMSD:\ttrain_MAE:\tval_RMSD:\tval_MAE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/graphnn/xjc/anaconda/envs/rdkit-env/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t6.987\t6.0806\t6.9559\t6.9181\n",
      "1\t6.405\t5.4108\t6.2875\t6.2456\n",
      "2\t4.6184\t3.361\t3.985\t3.9186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8625fc20fccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mSSE\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mSAE\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/xjc/anaconda/envs/rdkit-env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    454\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_RMSE_lis = []\n",
    "train_MAE_lis = []\n",
    "val_RMSE_lis = []\n",
    "val_MAE_lis = []\n",
    "test_RMSE_lis = []\n",
    "test_MAE_lis = []\n",
    "test_2_RMSE_lis = []\n",
    "test_2_MAE_lis = []\n",
    "cur_rmse_lis = []\n",
    "min_val_rmse = 100\n",
    "\n",
    "file_name = './Logger/try_B.txt'\n",
    "\n",
    "header = 'epoch:\\ttrain_RMSD:\\ttrain_MAE:\\tval_RMSD:\\tval_MAE:'\n",
    "print(header)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    for iter, (bg, label) in enumerate(train_loader):\n",
    "        bg = bg.to(device)\n",
    "        label = label.reshape(-1,1).to(device)\n",
    "        prediction = model(bg,bg.ndata['h'], bg.edata['h'])\n",
    "        loss = loss_func(prediction, label).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(),max_norm=20,norm_type=2)\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        SSE = 0\n",
    "        SAE = 0 \n",
    "        for iter, (bg, label) in enumerate(train_loader):\n",
    "            bg = bg.to(device)\n",
    "            prediction = model(bg,bg.ndata['h'], bg.edata['h'])\n",
    "            prediction = torch.squeeze(prediction)\n",
    "            label = label.to(device)\n",
    "            loss = prediction-label\n",
    "            SSE += sum(loss**2)\n",
    "            SAE += sum(torch.abs(loss))\n",
    "        N = len(train_loader.dataset)\n",
    "        train_RMSE = (SSE.item()/ N)**0.5\n",
    "        train_MAE = SAE.item()/N\n",
    "        #print(N,train_RMSD)\n",
    "\n",
    "        SSE = 0\n",
    "        SAE = 0 \n",
    "        for iter, (bg, label) in enumerate(val_loader):\n",
    "            bg = bg.to(device)\n",
    "            prediction = model(bg,bg.ndata['h'], bg.edata['h'])\n",
    "            prediction = torch.squeeze(prediction)\n",
    "            label = label.to(device)\n",
    "            loss = prediction-label\n",
    "            SSE += sum(loss**2)\n",
    "            SAE += sum(torch.abs(loss))\n",
    "        N = len(val_loader.dataset)\n",
    "        val_RMSE = (SSE.item()/ N)**0.5\n",
    "        val_MAE = SAE.item()/N\n",
    "\n",
    "        \n",
    "        \n",
    "        log = '{}\\t{}\\t{}\\t{}\\t{}'.format(epoch,round(train_RMSE,4),round(train_MAE,4),round(val_RMSE,4),round(val_MAE,4))\n",
    "        print(log)\n",
    "        \n",
    "        train_RMSE_lis.append(train_RMSE)\n",
    "        train_MAE_lis.append(train_MAE)\n",
    "        val_RMSE_lis.append(val_RMSE)\n",
    "        val_MAE_lis.append(val_MAE)\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "#         #save model\n",
    "#         if val_RMSE <= min_val_rmse:\n",
    "#             min_val_rmse = val_RMSE\n",
    "#             if epoch >= 300:\n",
    "#                 torch.save(model.state_dict(), './Trained_model/ramdom_split_acidic_4.pkl')\n",
    "#                 print('saved')\n",
    "\n",
    "#         with open(file_name,'a+') as f:\n",
    "#             f.write(log)\n",
    "#             f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate/10,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/graphnn/xjc/anaconda/envs/rdkit-env/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t0.5646\t0.3911\t0.628\t0.5514\n",
      "1001\t0.5626\t0.3887\t0.6603\t0.5763\n",
      "1002\t0.562\t0.3889\t0.6765\t0.5866\n",
      "1003\t0.559\t0.3851\t0.6616\t0.5767\n",
      "1004\t0.5613\t0.3883\t0.656\t0.5725\n",
      "1005\t0.5585\t0.3846\t0.6229\t0.5457\n",
      "1006\t0.5568\t0.3829\t0.6251\t0.5336\n",
      "1007\t0.5567\t0.3827\t0.6246\t0.5274\n",
      "1008\t0.5568\t0.3834\t0.6237\t0.5331\n",
      "1009\t0.5588\t0.3859\t0.6465\t0.5635\n",
      "1010\t0.557\t0.3828\t0.6336\t0.5481\n",
      "1011\t0.5563\t0.3822\t0.6062\t0.5191\n",
      "1012\t0.5558\t0.3818\t0.6096\t0.5283\n",
      "1013\t0.5545\t0.3803\t0.6314\t0.5509\n",
      "1014\t0.556\t0.3809\t0.655\t0.5588\n",
      "1015\t0.5541\t0.3803\t0.6158\t0.5353\n",
      "1016\t0.5592\t0.3859\t0.6157\t0.5229\n",
      "1017\t0.5543\t0.3802\t0.6242\t0.5369\n",
      "1018\t0.5541\t0.3804\t0.6126\t0.5258\n",
      "1019\t0.557\t0.3829\t0.6197\t0.525\n",
      "1020\t0.5561\t0.3823\t0.64\t0.5483\n",
      "1021\t0.5556\t0.3824\t0.6233\t0.5361\n",
      "1022\t0.5538\t0.3811\t0.6336\t0.5559\n",
      "1023\t0.5533\t0.3798\t0.6488\t0.5663\n",
      "1024\t0.5547\t0.3823\t0.6794\t0.5924\n",
      "1025\t0.5542\t0.3815\t0.6741\t0.588\n",
      "1026\t0.554\t0.3804\t0.6278\t0.5407\n",
      "1027\t0.5541\t0.381\t0.6203\t0.545\n",
      "1028\t0.5541\t0.3807\t0.672\t0.5821\n",
      "1029\t0.5541\t0.3811\t0.6341\t0.5543\n",
      "1030\t0.5531\t0.3793\t0.6285\t0.5422\n",
      "1031\t0.5526\t0.38\t0.6399\t0.554\n",
      "1032\t0.5529\t0.3796\t0.6465\t0.5608\n",
      "1033\t0.5521\t0.3787\t0.6634\t0.5741\n",
      "1034\t0.5545\t0.3813\t0.6453\t0.552\n",
      "1035\t0.5549\t0.3812\t0.6236\t0.5389\n",
      "1036\t0.5529\t0.3797\t0.6464\t0.5631\n",
      "1037\t0.5541\t0.3808\t0.6644\t0.5768\n",
      "1038\t0.5531\t0.3793\t0.6544\t0.5694\n",
      "1039\t0.5526\t0.3791\t0.6321\t0.5482\n",
      "1040\t0.552\t0.3795\t0.6431\t0.5635\n",
      "1041\t0.5525\t0.3791\t0.6227\t0.5457\n",
      "1042\t0.5513\t0.3781\t0.6323\t0.5524\n",
      "1043\t0.5522\t0.3791\t0.6216\t0.5396\n",
      "1044\t0.5516\t0.378\t0.6306\t0.5467\n",
      "1045\t0.5522\t0.3798\t0.6623\t0.577\n",
      "1046\t0.5514\t0.3777\t0.627\t0.5456\n",
      "1047\t0.5515\t0.3785\t0.6246\t0.5418\n",
      "1048\t0.5519\t0.3796\t0.6197\t0.5452\n",
      "1049\t0.553\t0.3812\t0.6434\t0.5669\n",
      "1050\t0.5545\t0.3828\t0.6185\t0.5447\n",
      "1051\t0.5624\t0.391\t0.6441\t0.5672\n",
      "1052\t0.5519\t0.3801\t0.6591\t0.5766\n",
      "1053\t0.5511\t0.3783\t0.6435\t0.5638\n",
      "1054\t0.5535\t0.3802\t0.6298\t0.5345\n",
      "1055\t0.5518\t0.378\t0.6528\t0.5592\n",
      "1056\t0.5517\t0.3805\t0.6302\t0.5531\n",
      "1057\t0.5551\t0.383\t0.6705\t0.5815\n",
      "1058\t0.5544\t0.3839\t0.6902\t0.5936\n",
      "1059\t0.5518\t0.3792\t0.6959\t0.5962\n",
      "1060\t0.5499\t0.3769\t0.6737\t0.5855\n",
      "1061\t0.5521\t0.3799\t0.6245\t0.5321\n",
      "1062\t0.5503\t0.3788\t0.6465\t0.5659\n",
      "1063\t0.55\t0.3781\t0.6746\t0.5871\n",
      "1064\t0.5495\t0.3767\t0.6539\t0.5683\n",
      "1065\t0.5501\t0.3777\t0.64\t0.558\n",
      "1066\t0.55\t0.3775\t0.6376\t0.554\n",
      "1067\t0.5559\t0.3842\t0.6316\t0.54\n",
      "1068\t0.5495\t0.3766\t0.6384\t0.5501\n",
      "1069\t0.5506\t0.3782\t0.6291\t0.5512\n",
      "1070\t0.5505\t0.3787\t0.6379\t0.5589\n",
      "1071\t0.553\t0.381\t0.6616\t0.5764\n",
      "1072\t0.5542\t0.383\t0.6589\t0.5765\n",
      "1073\t0.549\t0.3764\t0.6687\t0.5818\n",
      "1074\t0.5496\t0.377\t0.6492\t0.5648\n",
      "1075\t0.5493\t0.3772\t0.6416\t0.565\n",
      "1076\t0.5486\t0.3766\t0.6572\t0.5763\n",
      "1077\t0.5488\t0.3766\t0.6524\t0.5735\n",
      "1078\t0.5504\t0.378\t0.6691\t0.5852\n",
      "1079\t0.5494\t0.378\t0.6521\t0.5722\n",
      "1080\t0.5502\t0.3777\t0.6443\t0.5483\n",
      "1081\t0.5482\t0.3768\t0.6519\t0.572\n",
      "1082\t0.5492\t0.3769\t0.6786\t0.5929\n",
      "1083\t0.5546\t0.3853\t0.7081\t0.609\n",
      "1084\t0.5491\t0.3767\t0.6573\t0.5778\n",
      "1085\t0.5503\t0.3778\t0.6363\t0.5544\n",
      "1086\t0.5481\t0.3753\t0.6458\t0.5672\n",
      "1087\t0.5482\t0.3766\t0.6587\t0.5795\n",
      "1088\t0.552\t0.3813\t0.6714\t0.5873\n",
      "1089\t0.5527\t0.3827\t0.6562\t0.5759\n",
      "1090\t0.5487\t0.3767\t0.6376\t0.5596\n",
      "1091\t0.5479\t0.3767\t0.6747\t0.5878\n",
      "1092\t0.5472\t0.3751\t0.6534\t0.5715\n",
      "1093\t0.548\t0.3759\t0.6321\t0.5393\n",
      "1094\t0.5531\t0.3822\t0.6484\t0.5696\n",
      "1095\t0.5493\t0.3764\t0.6414\t0.5615\n",
      "1096\t0.5484\t0.3763\t0.6665\t0.5784\n",
      "1097\t0.5477\t0.375\t0.6415\t0.5352\n",
      "1098\t0.5511\t0.3788\t0.6371\t0.5278\n",
      "1099\t0.5501\t0.3782\t0.6272\t0.5246\n",
      "1100\t0.5479\t0.3752\t0.6422\t0.5501\n",
      "1101\t0.5471\t0.3745\t0.6611\t0.5681\n",
      "1102\t0.5518\t0.3797\t0.6928\t0.5974\n",
      "1103\t0.55\t0.3791\t0.6565\t0.5756\n",
      "1104\t0.5473\t0.3751\t0.6444\t0.5574\n",
      "1105\t0.5476\t0.3756\t0.6554\t0.566\n",
      "1106\t0.5519\t0.3789\t0.6522\t0.5639\n",
      "1107\t0.5477\t0.3756\t0.6345\t0.5514\n",
      "1108\t0.5463\t0.3738\t0.6367\t0.5459\n",
      "1109\t0.5459\t0.374\t0.6364\t0.5528\n",
      "1110\t0.5464\t0.3742\t0.6364\t0.5379\n",
      "1111\t0.5473\t0.3754\t0.6053\t0.5266\n",
      "1112\t0.5476\t0.375\t0.6256\t0.5427\n",
      "1113\t0.5468\t0.3748\t0.632\t0.5518\n",
      "1114\t0.5474\t0.3751\t0.6066\t0.5285\n",
      "1115\t0.5467\t0.3748\t0.6372\t0.5485\n",
      "1116\t0.5486\t0.3768\t0.6489\t0.5613\n",
      "1117\t0.5476\t0.3755\t0.6295\t0.5441\n",
      "1118\t0.5461\t0.3749\t0.6272\t0.5448\n",
      "1119\t0.5461\t0.3742\t0.658\t0.5737\n",
      "1120\t0.5462\t0.3741\t0.6328\t0.554\n",
      "1121\t0.5459\t0.3738\t0.6327\t0.5434\n",
      "1122\t0.5447\t0.3734\t0.6376\t0.5568\n",
      "1123\t0.5461\t0.3747\t0.6647\t0.575\n",
      "1124\t0.5446\t0.3734\t0.6339\t0.5356\n",
      "1125\t0.5469\t0.3752\t0.6422\t0.5414\n",
      "1126\t0.5471\t0.3759\t0.6352\t0.5466\n",
      "1127\t0.5457\t0.3748\t0.6439\t0.5643\n",
      "1128\t0.5505\t0.3797\t0.6784\t0.5908\n",
      "1129\t0.5483\t0.3792\t0.6816\t0.5939\n",
      "1130\t0.5479\t0.3756\t0.6356\t0.5529\n",
      "1131\t0.5491\t0.3797\t0.6817\t0.5937\n",
      "1132\t0.5482\t0.3758\t0.6814\t0.5938\n",
      "1133\t0.5483\t0.3776\t0.6758\t0.5912\n",
      "1134\t0.5493\t0.3775\t0.6814\t0.5945\n",
      "1135\t0.5473\t0.3776\t0.6785\t0.5913\n",
      "1136\t0.5457\t0.3742\t0.6605\t0.5654\n",
      "1137\t0.5477\t0.3759\t0.6467\t0.557\n",
      "1138\t0.5463\t0.3744\t0.6717\t0.5864\n",
      "1139\t0.5457\t0.3745\t0.6701\t0.585\n",
      "1140\t0.5481\t0.3763\t0.6513\t0.5692\n",
      "1141\t0.552\t0.3835\t0.6679\t0.5846\n",
      "1142\t0.5482\t0.377\t0.6346\t0.5565\n",
      "1143\t0.5462\t0.3767\t0.646\t0.5666\n",
      "1144\t0.5461\t0.3747\t0.6654\t0.5802\n",
      "1145\t0.5459\t0.3759\t0.6535\t0.5753\n",
      "1146\t0.5468\t0.3756\t0.6677\t0.5841\n",
      "1147\t0.5448\t0.3742\t0.6489\t0.5722\n",
      "1148\t0.5464\t0.3757\t0.6401\t0.5654\n",
      "1149\t0.5453\t0.3741\t0.7072\t0.6115\n",
      "1150\t0.5463\t0.3747\t0.6504\t0.5711\n",
      "1151\t0.5461\t0.376\t0.6383\t0.5651\n",
      "1152\t0.5456\t0.3746\t0.6447\t0.5682\n",
      "1153\t0.5499\t0.3783\t0.6424\t0.5481\n",
      "1154\t0.5455\t0.3745\t0.6154\t0.5327\n",
      "1155\t0.5458\t0.375\t0.6166\t0.5328\n",
      "1156\t0.5446\t0.3738\t0.6267\t0.551\n",
      "1157\t0.5437\t0.3728\t0.6405\t0.5633\n",
      "1158\t0.5452\t0.3744\t0.6461\t0.5643\n",
      "1159\t0.5437\t0.3727\t0.633\t0.5558\n",
      "1160\t0.5441\t0.3726\t0.6291\t0.5518\n",
      "1161\t0.5442\t0.3729\t0.6219\t0.5402\n",
      "1162\t0.5444\t0.373\t0.6364\t0.5533\n",
      "1163\t0.5447\t0.3742\t0.6433\t0.5626\n",
      "1164\t0.5475\t0.3776\t0.6352\t0.5601\n",
      "1165\t0.5534\t0.3841\t0.6225\t0.5493\n",
      "1166\t0.5531\t0.3847\t0.6333\t0.5589\n",
      "1167\t0.5437\t0.3731\t0.6389\t0.5574\n",
      "1168\t0.5439\t0.3741\t0.6191\t0.5442\n",
      "1169\t0.5439\t0.3732\t0.6257\t0.5469\n",
      "1170\t0.544\t0.373\t0.6369\t0.551\n",
      "1171\t0.5437\t0.3723\t0.642\t0.5575\n",
      "1172\t0.5429\t0.3718\t0.6644\t0.5742\n",
      "1173\t0.5439\t0.3723\t0.6599\t0.5688\n",
      "1174\t0.5438\t0.3734\t0.6469\t0.5596\n",
      "1175\t0.5451\t0.3747\t0.6394\t0.5595\n",
      "1176\t0.5444\t0.3739\t0.6608\t0.5785\n",
      "1177\t0.5438\t0.3734\t0.6515\t0.5682\n",
      "1178\t0.5437\t0.3724\t0.6762\t0.5798\n",
      "1179\t0.543\t0.372\t0.6679\t0.5813\n",
      "1180\t0.5431\t0.3723\t0.6703\t0.5858\n",
      "1181\t0.5434\t0.3728\t0.6731\t0.5871\n",
      "1182\t0.5452\t0.3758\t0.6676\t0.5843\n",
      "1183\t0.5458\t0.3755\t0.6613\t0.5781\n",
      "1184\t0.5467\t0.3786\t0.6835\t0.5943\n",
      "1185\t0.5448\t0.3758\t0.6723\t0.5888\n",
      "1186\t0.5452\t0.3758\t0.6919\t0.6006\n",
      "1187\t0.5427\t0.3723\t0.6497\t0.569\n",
      "1188\t0.5434\t0.3733\t0.6589\t0.5785\n",
      "1189\t0.5426\t0.3721\t0.648\t0.5672\n",
      "1190\t0.5429\t0.3723\t0.6321\t0.5381\n",
      "1191\t0.5424\t0.3716\t0.6456\t0.5445\n",
      "1192\t0.5431\t0.3722\t0.6476\t0.5556\n",
      "1193\t0.5428\t0.3725\t0.6397\t0.5526\n",
      "1194\t0.5422\t0.3714\t0.645\t0.5562\n",
      "1195\t0.5421\t0.3715\t0.6454\t0.5624\n",
      "1196\t0.542\t0.3714\t0.6344\t0.5458\n",
      "1197\t0.5442\t0.3733\t0.6397\t0.543\n",
      "1198\t0.5436\t0.3728\t0.6348\t0.5472\n",
      "1199\t0.5438\t0.3746\t0.6592\t0.5746\n",
      "1200\t0.5437\t0.3723\t0.6472\t0.5569\n",
      "1201\t0.5414\t0.3708\t0.6364\t0.5415\n",
      "1202\t0.5434\t0.3726\t0.6459\t0.5438\n",
      "1203\t0.5432\t0.3728\t0.6446\t0.552\n",
      "1204\t0.5423\t0.3709\t0.6404\t0.5402\n",
      "1205\t0.5415\t0.371\t0.6323\t0.5498\n",
      "1206\t0.5415\t0.3705\t0.6403\t0.5612\n",
      "1207\t0.545\t0.3758\t0.6536\t0.5744\n",
      "1208\t0.5432\t0.3721\t0.6266\t0.5421\n",
      "1209\t0.5424\t0.3727\t0.6252\t0.5494\n",
      "1210\t0.5449\t0.3746\t0.644\t0.5583\n",
      "1211\t0.5432\t0.3737\t0.6351\t0.5557\n",
      "1212\t0.5409\t0.3707\t0.6226\t0.5448\n",
      "1213\t0.5408\t0.3697\t0.6304\t0.5455\n",
      "1214\t0.5442\t0.3742\t0.6269\t0.5348\n",
      "1215\t0.5404\t0.3698\t0.6424\t0.555\n",
      "1216\t0.5412\t0.3715\t0.6398\t0.5625\n",
      "1217\t0.5423\t0.372\t0.6389\t0.561\n",
      "1218\t0.5423\t0.3738\t0.6629\t0.5797\n",
      "1219\t0.5423\t0.3722\t0.6684\t0.5803\n",
      "1220\t0.5424\t0.3734\t0.6508\t0.5682\n",
      "1221\t0.5431\t0.3742\t0.629\t0.5543\n",
      "1222\t0.5426\t0.3734\t0.6346\t0.5552\n",
      "1223\t0.5441\t0.3754\t0.6592\t0.5755\n",
      "1224\t0.5457\t0.3763\t0.6467\t0.567\n",
      "1225\t0.5425\t0.3739\t0.6351\t0.5587\n",
      "1226\t0.5403\t0.3703\t0.6233\t0.5392\n",
      "1227\t0.5397\t0.3692\t0.6261\t0.5341\n",
      "1228\t0.5405\t0.3703\t0.6312\t0.5339\n",
      "1229\t0.54\t0.3699\t0.6324\t0.5361\n",
      "1230\t0.5418\t0.3719\t0.6392\t0.5414\n",
      "1231\t0.5418\t0.3717\t0.6355\t0.5407\n",
      "1232\t0.541\t0.3714\t0.6577\t0.5715\n",
      "1233\t0.5413\t0.3715\t0.6468\t0.5661\n",
      "1234\t0.5408\t0.3713\t0.6285\t0.5364\n",
      "1235\t0.5402\t0.3697\t0.6258\t0.5311\n",
      "1236\t0.5401\t0.3705\t0.6361\t0.5574\n",
      "1237\t0.5415\t0.3721\t0.649\t0.5707\n",
      "1238\t0.54\t0.3694\t0.6478\t0.5677\n",
      "1239\t0.5391\t0.3699\t0.6441\t0.5672\n",
      "1240\t0.5412\t0.3713\t0.6457\t0.5593\n",
      "1241\t0.5394\t0.3696\t0.6403\t0.5588\n",
      "1242\t0.5469\t0.3786\t0.6639\t0.5808\n",
      "1243\t0.5403\t0.3712\t0.6358\t0.5569\n",
      "1244\t0.5439\t0.3757\t0.647\t0.5689\n",
      "1245\t0.5388\t0.368\t0.6396\t0.5501\n",
      "1246\t0.5397\t0.3687\t0.6332\t0.5392\n",
      "1247\t0.5412\t0.3715\t0.613\t0.5312\n",
      "1248\t0.5395\t0.3687\t0.646\t0.5498\n",
      "1249\t0.5388\t0.3691\t0.6376\t0.5522\n",
      "1250\t0.5386\t0.3694\t0.6183\t0.5398\n",
      "1251\t0.5406\t0.3704\t0.6304\t0.5313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1252\t0.5439\t0.3755\t0.6639\t0.5754\n",
      "1253\t0.5412\t0.3725\t0.626\t0.543\n",
      "1254\t0.5467\t0.3796\t0.6497\t0.5679\n",
      "1255\t0.5415\t0.3725\t0.6488\t0.5667\n",
      "1256\t0.5378\t0.3679\t0.6256\t0.5315\n",
      "1257\t0.5381\t0.3685\t0.6179\t0.5373\n",
      "1258\t0.5409\t0.3713\t0.633\t0.5551\n",
      "1259\t0.5401\t0.3705\t0.6491\t0.5699\n",
      "1260\t0.54\t0.3697\t0.617\t0.5366\n",
      "1261\t0.5416\t0.3721\t0.6334\t0.5521\n",
      "1262\t0.5397\t0.3697\t0.6352\t0.5558\n",
      "1263\t0.5392\t0.3697\t0.6371\t0.5553\n",
      "1264\t0.5402\t0.3711\t0.628\t0.5483\n",
      "1265\t0.54\t0.3701\t0.6224\t0.5403\n",
      "1266\t0.5382\t0.369\t0.6422\t0.5588\n",
      "1267\t0.5414\t0.3723\t0.6267\t0.5484\n",
      "1268\t0.5396\t0.3699\t0.6338\t0.5361\n",
      "1269\t0.5404\t0.3709\t0.6234\t0.5414\n",
      "1270\t0.5403\t0.3716\t0.6409\t0.5563\n",
      "1271\t0.5424\t0.3732\t0.6228\t0.5362\n",
      "1272\t0.5378\t0.3688\t0.6439\t0.559\n",
      "1273\t0.5392\t0.3703\t0.6391\t0.5559\n",
      "1274\t0.538\t0.3686\t0.629\t0.5378\n",
      "1275\t0.5388\t0.3697\t0.6449\t0.559\n",
      "1276\t0.5392\t0.3708\t0.6375\t0.5573\n",
      "1277\t0.5382\t0.3696\t0.6453\t0.5633\n",
      "1278\t0.5379\t0.3693\t0.6433\t0.5641\n",
      "1279\t0.5386\t0.3687\t0.6394\t0.5533\n",
      "1280\t0.5375\t0.3685\t0.6472\t0.5604\n",
      "1281\t0.5438\t0.3752\t0.674\t0.5833\n",
      "1282\t0.5447\t0.377\t0.6681\t0.5821\n",
      "1283\t0.5383\t0.3696\t0.6797\t0.5891\n",
      "1284\t0.5372\t0.3682\t0.6901\t0.5965\n",
      "1285\t0.5377\t0.3684\t0.6546\t0.5735\n",
      "1286\t0.5383\t0.3683\t0.638\t0.5421\n",
      "1287\t0.5373\t0.368\t0.642\t0.5554\n",
      "1288\t0.5373\t0.3679\t0.6683\t0.5819\n",
      "1289\t0.5381\t0.3678\t0.6884\t0.5934\n",
      "1290\t0.5375\t0.3682\t0.663\t0.5759\n",
      "1291\t0.5404\t0.3715\t0.6686\t0.5825\n",
      "1292\t0.5421\t0.3721\t0.6748\t0.5788\n",
      "1293\t0.5375\t0.3677\t0.6748\t0.5877\n",
      "1294\t0.5373\t0.3677\t0.6539\t0.5694\n",
      "1295\t0.5388\t0.3705\t0.6645\t0.5797\n",
      "1296\t0.5488\t0.3822\t0.6919\t0.5986\n",
      "1297\t0.5421\t0.3746\t0.6574\t0.5726\n",
      "1298\t0.5422\t0.3754\t0.6999\t0.6039\n",
      "1299\t0.5389\t0.371\t0.6997\t0.607\n",
      "1300\t0.5396\t0.3714\t0.6666\t0.5806\n",
      "1301\t0.5363\t0.3672\t0.6398\t0.5506\n",
      "1302\t0.5378\t0.3687\t0.6478\t0.5665\n",
      "1303\t0.5376\t0.3689\t0.6251\t0.5459\n",
      "1304\t0.5364\t0.3667\t0.6217\t0.5387\n",
      "1305\t0.5357\t0.3663\t0.6245\t0.5405\n",
      "1306\t0.5359\t0.3669\t0.6301\t0.5434\n",
      "1307\t0.5386\t0.3702\t0.6539\t0.5699\n",
      "1308\t0.5376\t0.3693\t0.6766\t0.5907\n",
      "1309\t0.5382\t0.3692\t0.6779\t0.5898\n",
      "1310\t0.5373\t0.3685\t0.6418\t0.563\n",
      "1311\t0.5382\t0.3702\t0.6641\t0.5811\n",
      "1312\t0.541\t0.3722\t0.6912\t0.5987\n",
      "1313\t0.5427\t0.3755\t0.7536\t0.646\n",
      "1314\t0.5373\t0.3683\t0.7419\t0.6353\n",
      "1315\t0.5399\t0.3703\t0.6748\t0.5874\n",
      "1316\t0.5392\t0.3698\t0.644\t0.5585\n",
      "1317\t0.5364\t0.3673\t0.6374\t0.5572\n",
      "1318\t0.5366\t0.3676\t0.6267\t0.5418\n",
      "1319\t0.5354\t0.366\t0.6457\t0.559\n",
      "1320\t0.5359\t0.3667\t0.636\t0.5544\n",
      "1321\t0.5364\t0.3674\t0.6329\t0.5471\n",
      "1322\t0.5354\t0.366\t0.6555\t0.5649\n",
      "1323\t0.5412\t0.3725\t0.6593\t0.5616\n",
      "1324\t0.5357\t0.3669\t0.6508\t0.5629\n",
      "1325\t0.536\t0.3665\t0.638\t0.5476\n",
      "1326\t0.537\t0.3671\t0.6231\t0.5347\n",
      "1327\t0.5358\t0.3661\t0.617\t0.5267\n",
      "1328\t0.5369\t0.3671\t0.6262\t0.5347\n",
      "1329\t0.5421\t0.3736\t0.6263\t0.5344\n",
      "1330\t0.5369\t0.3673\t0.638\t0.5455\n",
      "1331\t0.5367\t0.3672\t0.6339\t0.541\n",
      "1332\t0.5366\t0.3668\t0.6293\t0.54\n",
      "1333\t0.5374\t0.3678\t0.6182\t0.53\n",
      "1334\t0.5361\t0.3665\t0.6418\t0.5526\n",
      "1335\t0.5358\t0.3666\t0.6247\t0.5411\n",
      "1336\t0.5445\t0.3765\t0.6443\t0.5659\n",
      "1337\t0.5409\t0.3733\t0.6679\t0.5823\n",
      "1338\t0.5354\t0.3666\t0.6601\t0.5735\n",
      "1339\t0.5367\t0.3669\t0.6529\t0.5581\n",
      "1340\t0.5354\t0.3666\t0.6477\t0.5641\n",
      "1341\t0.5379\t0.3689\t0.6389\t0.5429\n",
      "1342\t0.5392\t0.3695\t0.6437\t0.5491\n",
      "1343\t0.5429\t0.3744\t0.6439\t0.5476\n",
      "1344\t0.5348\t0.3654\t0.638\t0.5441\n",
      "1345\t0.5401\t0.372\t0.6348\t0.5577\n",
      "1346\t0.5414\t0.3738\t0.6717\t0.5871\n",
      "1347\t0.5348\t0.3648\t0.6378\t0.5491\n",
      "1348\t0.5374\t0.3681\t0.6287\t0.5382\n",
      "1349\t0.5423\t0.3737\t0.6344\t0.5415\n",
      "1350\t0.5361\t0.3668\t0.6278\t0.5356\n",
      "1351\t0.5342\t0.3652\t0.6274\t0.5393\n",
      "1352\t0.5354\t0.3674\t0.634\t0.554\n",
      "1353\t0.5366\t0.3688\t0.6503\t0.5703\n",
      "1354\t0.5353\t0.367\t0.6636\t0.5806\n",
      "1355\t0.5352\t0.3669\t0.6305\t0.5546\n",
      "1356\t0.5383\t0.3713\t0.6459\t0.5691\n",
      "1357\t0.535\t0.3679\t0.6623\t0.5788\n",
      "1358\t0.5381\t0.3701\t0.6739\t0.5917\n",
      "1359\t0.5367\t0.369\t0.6544\t0.5729\n",
      "1360\t0.5343\t0.3656\t0.6237\t0.541\n",
      "1361\t0.5348\t0.366\t0.6288\t0.5497\n",
      "1362\t0.537\t0.3696\t0.6757\t0.5891\n",
      "1363\t0.5386\t0.3701\t0.6651\t0.5816\n",
      "1364\t0.5382\t0.3716\t0.6802\t0.5926\n",
      "1365\t0.5343\t0.3654\t0.6665\t0.5823\n",
      "1366\t0.5354\t0.3667\t0.6252\t0.5475\n",
      "1367\t0.5365\t0.3676\t0.624\t0.5402\n",
      "1368\t0.5354\t0.3676\t0.6369\t0.561\n",
      "1369\t0.5352\t0.3673\t0.6711\t0.5879\n",
      "1370\t0.5358\t0.3669\t0.6498\t0.5643\n",
      "1371\t0.5358\t0.3669\t0.6303\t0.5359\n",
      "1372\t0.5346\t0.3658\t0.6508\t0.5642\n",
      "1373\t0.5335\t0.3652\t0.6279\t0.5346\n",
      "1374\t0.5347\t0.3669\t0.628\t0.5373\n",
      "1375\t0.5413\t0.3745\t0.6606\t0.5789\n",
      "1376\t0.5429\t0.3779\t0.6703\t0.5856\n",
      "1377\t0.5354\t0.3664\t0.6461\t0.5461\n",
      "1378\t0.5349\t0.3666\t0.641\t0.5453\n",
      "1379\t0.5344\t0.3655\t0.6413\t0.5508\n",
      "1380\t0.5339\t0.3654\t0.6334\t0.5415\n",
      "1381\t0.5337\t0.3658\t0.6495\t0.5572\n",
      "1382\t0.5363\t0.3674\t0.668\t0.5757\n",
      "1383\t0.5366\t0.3703\t0.672\t0.5847\n",
      "1384\t0.5341\t0.3657\t0.6435\t0.5489\n",
      "1385\t0.5344\t0.3657\t0.6379\t0.5434\n",
      "1386\t0.5341\t0.3658\t0.6695\t0.5795\n",
      "1387\t0.534\t0.3657\t0.647\t0.5592\n",
      "1388\t0.5381\t0.3705\t0.6243\t0.5393\n",
      "1389\t0.5361\t0.3676\t0.6185\t0.532\n",
      "1390\t0.5354\t0.3677\t0.637\t0.5559\n",
      "1391\t0.5316\t0.3635\t0.6609\t0.5764\n",
      "1392\t0.5321\t0.3634\t0.6372\t0.5535\n",
      "1393\t0.5328\t0.3641\t0.6459\t0.5603\n",
      "1394\t0.533\t0.3648\t0.6329\t0.5519\n",
      "1395\t0.5328\t0.3647\t0.6232\t0.5367\n",
      "1396\t0.5322\t0.3644\t0.6397\t0.5533\n",
      "1397\t0.5323\t0.3643\t0.6375\t0.5482\n",
      "1398\t0.5322\t0.3643\t0.6328\t0.538\n",
      "1399\t0.5327\t0.3638\t0.6312\t0.5399\n",
      "1400\t0.5336\t0.3663\t0.6428\t0.5644\n",
      "1401\t0.5324\t0.3642\t0.6433\t0.5625\n",
      "1402\t0.5353\t0.3669\t0.6364\t0.5398\n",
      "1403\t0.534\t0.3661\t0.6293\t0.542\n",
      "1404\t0.5339\t0.3661\t0.64\t0.5579\n",
      "1405\t0.5321\t0.3643\t0.6363\t0.5457\n",
      "1406\t0.5369\t0.369\t0.6371\t0.5512\n",
      "1407\t0.533\t0.3659\t0.6403\t0.5582\n",
      "1408\t0.5364\t0.3689\t0.6416\t0.5594\n",
      "1409\t0.536\t0.3687\t0.6496\t0.569\n",
      "1410\t0.538\t0.371\t0.6563\t0.574\n",
      "1411\t0.538\t0.3725\t0.6451\t0.5655\n",
      "1412\t0.5321\t0.3641\t0.6295\t0.5478\n",
      "1413\t0.5354\t0.3663\t0.6383\t0.5413\n",
      "1414\t0.5334\t0.365\t0.636\t0.541\n",
      "1415\t0.5321\t0.364\t0.6315\t0.5415\n",
      "1416\t0.5359\t0.37\t0.6491\t0.5644\n",
      "1417\t0.5362\t0.3696\t0.6499\t0.5671\n",
      "1418\t0.5366\t0.3697\t0.6417\t0.557\n",
      "1419\t0.5323\t0.3647\t0.6357\t0.5523\n",
      "1420\t0.5347\t0.3672\t0.6535\t0.5706\n",
      "1421\t0.5314\t0.3641\t0.632\t0.5481\n",
      "1422\t0.5316\t0.3643\t0.6118\t0.5266\n",
      "1423\t0.5324\t0.3657\t0.6267\t0.5417\n",
      "1424\t0.532\t0.3647\t0.6355\t0.5532\n",
      "1425\t0.5317\t0.3645\t0.6393\t0.5581\n",
      "1426\t0.5333\t0.3672\t0.6501\t0.5688\n",
      "1427\t0.532\t0.3644\t0.6335\t0.5546\n",
      "1428\t0.5315\t0.3631\t0.6452\t0.5582\n",
      "1429\t0.5316\t0.364\t0.6573\t0.5727\n",
      "1430\t0.5319\t0.3644\t0.677\t0.589\n",
      "1431\t0.5316\t0.3638\t0.6425\t0.5581\n",
      "1432\t0.5308\t0.3625\t0.6523\t0.5682\n",
      "1433\t0.5317\t0.3646\t0.6405\t0.5597\n",
      "1434\t0.5323\t0.3641\t0.6316\t0.5407\n",
      "1435\t0.531\t0.363\t0.6395\t0.5549\n",
      "1436\t0.5304\t0.3626\t0.6318\t0.5467\n",
      "1437\t0.5305\t0.3635\t0.6407\t0.5597\n",
      "1438\t0.5307\t0.3629\t0.6415\t0.5496\n",
      "1439\t0.5318\t0.3644\t0.6223\t0.5321\n",
      "1440\t0.5319\t0.3638\t0.6242\t0.5322\n",
      "1441\t0.5322\t0.3635\t0.6366\t0.5387\n",
      "1442\t0.5306\t0.3621\t0.6518\t0.5591\n",
      "1443\t0.5316\t0.3633\t0.6548\t0.567\n",
      "1444\t0.531\t0.3632\t0.6429\t0.5562\n",
      "1445\t0.5313\t0.3639\t0.6488\t0.5603\n",
      "1446\t0.5306\t0.3632\t0.647\t0.5521\n",
      "1447\t0.5312\t0.3645\t0.6667\t0.5765\n",
      "1448\t0.5342\t0.3667\t0.6712\t0.5791\n",
      "1449\t0.5318\t0.3659\t0.6436\t0.5607\n",
      "1450\t0.5338\t0.3671\t0.6523\t0.567\n",
      "1451\t0.5325\t0.366\t0.6715\t0.5811\n",
      "1452\t0.5346\t0.3688\t0.6906\t0.595\n",
      "1453\t0.5318\t0.3644\t0.6545\t0.5696\n",
      "1454\t0.5357\t0.3699\t0.6725\t0.583\n",
      "1455\t0.5358\t0.3683\t0.6834\t0.5891\n",
      "1456\t0.5362\t0.3697\t0.6654\t0.5779\n",
      "1457\t0.5315\t0.3637\t0.6497\t0.5609\n",
      "1458\t0.5301\t0.3619\t0.678\t0.5868\n",
      "1459\t0.5301\t0.3627\t0.6659\t0.5792\n",
      "1460\t0.5296\t0.3623\t0.6711\t0.585\n",
      "1461\t0.5303\t0.3627\t0.6654\t0.5792\n",
      "1462\t0.5296\t0.3619\t0.683\t0.5917\n",
      "1463\t0.53\t0.362\t0.6468\t0.563\n",
      "1464\t0.5325\t0.3656\t0.6274\t0.5462\n",
      "1465\t0.5312\t0.3649\t0.7069\t0.6047\n",
      "1466\t0.5308\t0.3634\t0.6971\t0.5971\n",
      "1467\t0.5309\t0.3634\t0.6656\t0.5768\n",
      "1468\t0.5315\t0.3638\t0.6571\t0.5696\n",
      "1469\t0.5302\t0.363\t0.6741\t0.5848\n",
      "1470\t0.5304\t0.3633\t0.6547\t0.568\n",
      "1471\t0.5309\t0.3644\t0.6557\t0.5713\n",
      "1472\t0.5322\t0.3649\t0.6506\t0.5641\n",
      "1473\t0.5347\t0.3683\t0.6649\t0.5761\n",
      "1474\t0.5347\t0.3687\t0.6526\t0.5683\n",
      "1475\t0.5333\t0.3664\t0.646\t0.5631\n",
      "1476\t0.5311\t0.3643\t0.6369\t0.5509\n",
      "1477\t0.533\t0.3659\t0.6598\t0.5741\n",
      "1478\t0.5322\t0.3657\t0.6722\t0.5851\n",
      "1479\t0.5329\t0.3667\t0.6634\t0.5804\n",
      "1480\t0.5351\t0.3692\t0.6768\t0.5899\n",
      "1481\t0.5392\t0.3735\t0.6857\t0.5944\n",
      "1482\t0.5299\t0.3633\t0.6922\t0.5989\n",
      "1483\t0.5351\t0.3694\t0.697\t0.6008\n",
      "1484\t0.5381\t0.372\t0.6742\t0.5851\n",
      "1485\t0.5422\t0.3779\t0.7146\t0.6102\n",
      "1486\t0.5317\t0.3643\t0.6549\t0.5685\n",
      "1487\t0.5297\t0.3621\t0.6442\t0.549\n",
      "1488\t0.5304\t0.3631\t0.6404\t0.5518\n",
      "1489\t0.5314\t0.3639\t0.6556\t0.573\n",
      "1490\t0.5286\t0.361\t0.6613\t0.574\n",
      "1491\t0.5287\t0.3604\t0.6466\t0.561\n",
      "1492\t0.5302\t0.3628\t0.649\t0.5654\n",
      "1493\t0.5297\t0.3612\t0.6328\t0.5419\n",
      "1494\t0.5291\t0.3608\t0.6446\t0.5588\n",
      "1495\t0.5306\t0.3622\t0.6409\t0.5593\n",
      "1496\t0.5298\t0.3624\t0.657\t0.5735\n",
      "1497\t0.5286\t0.3614\t0.635\t0.5442\n",
      "1498\t0.5293\t0.3617\t0.6319\t0.549\n",
      "1499\t0.5283\t0.3611\t0.6543\t0.5687\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000,1500):\n",
    "    model.train()\n",
    "    for iter, (bg, label) in enumerate(train_loader):\n",
    "        bg = bg.to(device)\n",
    "        label = label.reshape(-1,1).to(device)\n",
    "        prediction = model(bg,bg.ndata['h'], bg.edata['h'])\n",
    "        loss = loss_func(prediction, label).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(),max_norm=20,norm_type=2)\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        SSE = 0\n",
    "        SAE = 0 \n",
    "        for iter, (bg, label) in enumerate(train_loader):\n",
    "            bg = bg.to(device)\n",
    "            prediction = model(bg,bg.ndata['h'], bg.edata['h'])\n",
    "            prediction = torch.squeeze(prediction)\n",
    "            label = label.to(device)\n",
    "            loss = prediction-label\n",
    "            SSE += sum(loss**2)\n",
    "            SAE += sum(torch.abs(loss))\n",
    "        N = len(train_loader.dataset)\n",
    "        train_RMSE = (SSE.item()/ N)**0.5\n",
    "        train_MAE = SAE.item()/N\n",
    "        #print(N,train_RMSD)\n",
    "\n",
    "        SSE = 0\n",
    "        SAE = 0 \n",
    "        for iter, (bg, label) in enumerate(val_loader):\n",
    "            bg = bg.to(device)\n",
    "            prediction = model(bg,bg.ndata['h'], bg.edata['h'])\n",
    "            prediction = torch.squeeze(prediction)\n",
    "            label = label.to(device)\n",
    "            loss = prediction-label\n",
    "            SSE += sum(loss**2)\n",
    "            SAE += sum(torch.abs(loss))\n",
    "        N = len(val_loader.dataset)\n",
    "        val_RMSE = (SSE.item()/ N)**0.5\n",
    "        val_MAE = SAE.item()/N\n",
    "\n",
    "        \n",
    "        \n",
    "        log = '{}\\t{}\\t{}\\t{}\\t{}'.format(epoch,round(train_RMSE,4),round(train_MAE,4),round(val_RMSE,4),round(val_MAE,4))\n",
    "        print(log)\n",
    "        \n",
    "        train_RMSE_lis.append(train_RMSE)\n",
    "        train_MAE_lis.append(train_MAE)\n",
    "        val_RMSE_lis.append(val_RMSE)\n",
    "        val_MAE_lis.append(val_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './Trained_model/non_B_try_BOB.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般到后面才开始学smalp6?\n",
    "\n",
    "这份，非常稳定，但感觉值偏大？\n",
    "不能降到0.5？\n",
    "\n",
    "\n",
    "这个感觉还算稳定了\n",
    "\n",
    "\n",
    "先看看模型的泛化能力再决定要不要主动做聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "感觉不稳定的测试还是尽量避免"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
